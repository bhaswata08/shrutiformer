{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from typing import List, Literal, TypedDict, Any\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain.output_parsers import (\n",
    "    PydanticOutputParser, \n",
    "    OutputFixingParser, \n",
    "    RetryOutputParser\n",
    "    )\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "class BlobClassifier(BaseModel):\n",
    "    blob : List[Literal['dry', 'parent', 'sign', 'hard', 'screen', 'page', 'exercise', 'healthy', 'doctor', 'bedroom', 'window', 'letter', 'narrow', 'small little', 'university', 'brown', 'ball', 'evening', 'race (ethnicity)', 'colour', 'war', 'beautiful', 'hospital', 'low', 'train', 'book', 'loud', 'priest', 'sunday', 'pink', 'skirt', 'summer', 'clock', 'he', 'sister', 'year', 'adult', 'crowd', 'month', 'student', 'school', 'lamp', 'president', 'long', 'car', 'dirty', 'peace', 'bill', 'suit', 'fan', 'author', 'boat', 'we', 'box', 'rich', 'alright', 'soldier', 'female', 'technology', 'radio', 'afternoon', 'bag', 'tomorrow', 'pant', 'slow', 'warm', 'player', 'energy', 'marriage', 'father', 'black', 'old', 'shallow', 'yesterday', 'patient', 'court', 'secretary', 'cell phone', 'baby', 'computer', 't-shirt', 'cat', 'soft', 'mother', 'medicine', 'green', 'girl', 'good evening', 'monday', 'sad', 'today', 'light', 'cold', 'soap', 'high', 'hot', 'bad', 'heavy', 'pleased', 'tool', 'orange', 'fall', 'happy', 'tight', 'it', 'animal', 'week', 'night', 'hello', 'child', 'religion', 'chair', 'wednesday', 'son', 'blue', 'bird', 'truck', 'ugly', 'paint', 'loose', 'they', 'hour', 'alive', 'neighbour', 'reporter', 'young', 'mean', 'deep', 'paper', 'waiter', 'teacher', 'laptop', 'god', 'sick', 'good morning', 'lock', 'gun', 'bank', 'photograph', 'minute', 'price', 'brother', 'male', 'daughter', 'job', 'deaf', 'door', 'telephone', 'short', 'king', 'ring', 'pencil', 'pen', 'city', 'kitchen', 'bathroom', 'india', 'train ticket', 'bed', 'how are you', 'dog', 'actor', 'fast', 'artist', 'time', 'tuesday', 'weak', 'horse', 'red', 'dress', 'transportation', 'bicycle', 'you', 'strong', 'dream', 'mouse', 'husband', 'thursday', 'winter', 'thick', 'good afternoon', 'park', 'key', 'woman', 'sport', 'police', 'market', 'flat', 'cow', 'street or road', 'restaurant', 'grey', 'dead', 'boy', 'family', 'cheap', 'card', 'white', 'season', 'monsoon', 'team', 'newspaper', 'grandmother', 'queen', 'good night', 'thank you', 'money', 'cool', 'spring', 'office', 'clean', 'hat', 'second', 'science', 'clothing', 'expensive', 'grandfather', 'plane', 'table', 'friday', 'election', 'i', 'television', 'wet', 'friend', 'wife', 'she', 'shirt', 'lawyer', 'famous', 'death', 'blind', 'gift', 'yellow', 'curved', 'train station', 'big large', 'poor', 'pocket', 'ground', 'man', 'temple', 'saturday', 'tall', 'location', 'manager', 'nice', 'house', 'attack', 'library', 'wide', 'store', 'quiet', 'camera', 'bus', 'fish', 'shoes', 'morning', 'thin', 'good', 'new']]\n",
    "    cot_reason: str = Field(..., description=\"Step by step breakdown of your reason for the blob output\")\n",
    "    cross_verification : bool = Field(..., description=\"Have you Double checked your output, and crossverifed if it matches the output schema?\")\n",
    "\n",
    "from zukilangchain import CustomLM\n",
    "llm = CustomLM(api_key=\"zu-<ZUKI_API_KEY>\", base_url=\"https://zukijourney.xyzbot.net/v1\", model_name=\"mixtral8x7b\")\n",
    "from ffmpeg_merge import merge_videos\n",
    "\n",
    "\n",
    "\n",
    "prompt = '''<instructions>\n",
    "You are an expert sign language translator, Based on the user query, your task is to divide the user query into blobs of text.\n",
    "Do not output anything other than the text blobs in json schema.\n",
    "1. Only output in the specified format without any preamble or extra information.\n",
    "2. The categories must be only and only from the given categories.\n",
    "3. The categories must be in lowercase.\n",
    "4. Cross verify your results\n",
    "5. It is okay if the sentence doesnt have some words, you can skip out in your response\n",
    "6. I will give you $1000 for the correct output and fine you $4000 for the wrong output.\n",
    "7. Strictly adhere to the standards for formatting, dont output \"Here is the given json schema, <json schema>\" \n",
    "8. Do not hallucinate up categories, use categories from the list available.\n",
    "</instructions>\n",
    "<examples>\n",
    "query = \"The doctor signed the letter.\"\n",
    "Your response: [\"doctor\", \"sign\", \"letter\"]\n",
    "query = \"Hi, How are you doing\"\n",
    "Your response: [\"how are you\"]\n",
    "query = \"I'll call you tomorrow after work.\"\n",
    "Your response: [\"cell phone\", \"tomorrow\"]\n",
    "</examples>\n",
    "<format instructions>\n",
    "\\n{format_instructions}\n",
    "</format instructions>\n",
    "<user query>\n",
    "\\n{query}\\n\n",
    "</user query>\n",
    "'''\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    blob_output: BlobClassifier\n",
    "    videos: Any\n",
    "\n",
    "def blob_divider(state):\n",
    "    query = state['query']\n",
    "    parser = PydanticOutputParser(pydantic_object=BlobClassifier)\n",
    "    prompt_temp = PromptTemplate(\n",
    "        template=prompt,\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    completion_chain = prompt_temp | llm | parser\n",
    "    out = completion_chain.invoke(query)\n",
    "    state['blob_output'] = out\n",
    "    return state\n",
    "\n",
    "def media_parser(state):\n",
    "    print(state['blob_output'])\n",
    "    blobs = state['blob_output'].blob\n",
    "    \n",
    "    task_id=\"testtask4\"\n",
    "    vids = merge_videos(task_id, blobs)\n",
    "    state['videos'] = vids\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"blob_divider\", blob_divider)\n",
    "workflow.add_node(\"media_parser\", media_parser)\n",
    "\n",
    "workflow.set_entry_point(\"blob_divider\")\n",
    "workflow.add_edge(\"blob_divider\", \"media_parser\")\n",
    "workflow.add_edge(\"media_parser\", END)\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to execute the cell below for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'blob_divider':\n",
      "---\n",
      "{'query': 'The doctor signed the letter.', 'blob_output': BlobClassifier(blob=['doctor', 'sign', 'letter'], cot_reason='The query \"The doctor signed the letter.\" contains three relevant words that match categories from the provided list: \"doctor\", \"sign\", and \"letter\". So I extracted these three words into the \"blob\" array.', cross_verification=True), 'videos': None}\n",
      "\n",
      "---\n",
      "\n",
      "blob=['doctor', 'sign', 'letter'] cot_reason='The query \"The doctor signed the letter.\" contains three relevant words that match categories from the provided list: \"doctor\", \"sign\", and \"letter\". So I extracted these three words into the \"blob\" array.' cross_verification=True\n",
      "Moviepy - Building video /home/bhaswata08/Self Projects/merged/t_e_s_t_t_a_s_k_4.mov.\n",
      "Moviepy - Writing video /home/bhaswata08/Self Projects/merged/t_e_s_t_t_a_s_k_4.mov\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/bhaswata08/Self Projects/merged/t_e_s_t_t_a_s_k_4.mov\n",
      "Output from node 'media_parser':\n",
      "---\n",
      "{'query': 'The doctor signed the letter.', 'blob_output': BlobClassifier(blob=['doctor', 'sign', 'letter'], cot_reason='The query \"The doctor signed the letter.\" contains three relevant words that match categories from the provided list: \"doctor\", \"sign\", and \"letter\". So I extracted these three words into the \"blob\" array.', cross_verification=True), 'videos': {'path': '/home/bhaswata08/Self Projects/merged/t_e_s_t_t_a_s_k_4.mov'}}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node '__end__':\n",
      "---\n",
      "{'query': 'The doctor signed the letter.', 'blob_output': BlobClassifier(blob=['doctor', 'sign', 'letter'], cot_reason='The query \"The doctor signed the letter.\" contains three relevant words that match categories from the provided list: \"doctor\", \"sign\", and \"letter\". So I extracted these three words into the \"blob\" array.', cross_verification=True), 'videos': {'path': '/home/bhaswata08/Self Projects/merged/t_e_s_t_t_a_s_k_4.mov'}}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"query\": \"The doctor signed the letter.\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
